Title:
DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability
URL:
https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.pdf

Quantiative Experiment Goal:
Table 1:
Row4: DiffDis on CIFAR10 and CIFAR100 datasets

Qualitative Experiment Goal:
Figure 4. Qualitative comparisons of Stable diffusion and DiffDis on MSCOCO zero-shot text-to-image generation.

1. We did not change our experimental goals
2. To validate our model's training, we're using a dummy dataset that mimics the original CC3M dataset. Due to the CC3M's large size (~430 GB) and our resource constraints, completing training before the May 5 deadline for the first version is unfeasible. However, we aim to finish training by the second version's deadline on May 31.
3. We've discovered that the transformer for the image-to-text task is a separate entity from our UNet model. It consists of self-attention blocks, indicating it could be constructed using UNet's self-attention blocks for a simpler implementation. However, this approach is still being considered.
